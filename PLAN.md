# Aztec Butler - Architecture & Implementation Plan

## Current State

Single CLI tool with manual commands for Aztec node admin operations.

## Target State

Two execution modes:

1. **CLI mode** - Individual commands (current functionality)
2. **Server mode** - Combined Prometheus exporter + Event watcher

---

## Architecture Decision: Combined Server Mode

### âœ… Combined Prometheus + Watcher - RIGHT CHOICE FOR THIS PROJECT

**We're combining Prometheus exporter and event watcher into a single server process because:**

1. **Shared Data Sources**
   - Both need to scrape the same sources (on-chain data, local node files)
   - Prometheus exposes current state as metrics
   - Watcher monitors the same state for changes
   - No point fetching the same data twice from two processes

2. **Simpler Deployment**
   - Single process to manage and monitor
   - One container/service to deploy
   - Single configuration file
   - One health check endpoint
   - Fewer moving parts = more reliable

3. **Resource Efficiency**
   - Single RPC connection to Ethereum/Aztec nodes
   - One set of file system watchers
   - Shared in-memory state between components
   - Lower overhead on the host system

4. **Appropriate Scale**
   - Managing individual Aztec nodes, not multi-tenant platform
   - Not running hundreds of instances
   - Resource contention won't be an issue
   - Brief restarts are acceptable (Prometheus can tolerate short downtime)

5. **Unified Lifecycle**
   - Both are long-running daemons with same uptime requirements
   - If either fails, restarting both together makes sense
   - Shared error handling and recovery logic
   - Single graceful shutdown procedure

### Why Not Separate Processes?

The separation would be good engineering for large-scale systems, but introduces unnecessary complexity for this use case:

- No need to scale components independently
- Same failure recovery strategy for both
- Added operational overhead with no real benefit

### Binary Design

```bash
aztec-butler <command>              # CLI mode (one-shot commands)
aztec-butler serve                  # Server mode (Prometheus + Watcher)
```

This gives you:

- Single build artifact
- Shared code/config between modes
- Simple deployment story
- Can still split later if needed (YAGNI principle)

---

## Proposed Repository Layout

```
aztec-butler/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli/                       # CLI command implementations
â”‚   â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”‚   â”œâ”€â”€ deposit-calldata.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ get-publisher-eth.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ attester-registration.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ provider-management.ts
â”‚   â”‚   â”‚   â””â”€â”€ index.ts           # Export all commands
â”‚   â”‚   â”œâ”€â”€ index.ts               # CLI entry point & arg parsing
â”‚   â”‚   â””â”€â”€ runner.ts              # Execute individual commands
â”‚   â”‚
â”‚   â”œâ”€â”€ server/                    # Combined server mode (Prometheus + Watcher)
â”‚   â”‚   â”œâ”€â”€ index.ts               # Server entry point (starts both HTTP + watchers)
â”‚   â”‚   â”œâ”€â”€ http-server.ts         # HTTP server for /metrics and /health
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ scrapers/              # Data collection from sources
â”‚   â”‚   â”‚   â”œâ”€â”€ base-scraper.ts    # Abstract scraper interface
â”‚   â”‚   â”‚   â”œâ”€â”€ node-scraper.ts    # Aztec node data (files + RPC)
â”‚   â”‚   â”‚   â”œâ”€â”€ l1-scraper.ts      # L1 contract state
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ metrics/               # Prometheus metrics exposure
â”‚   â”‚   â”‚   â”œâ”€â”€ registry.ts        # Central metrics registry
â”‚   â”‚   â”‚   â”œâ”€â”€ node-metrics.ts    # Node-related metrics
â”‚   â”‚   â”‚   â”œâ”€â”€ l1-metrics.ts      # L1 contract metrics
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ watchers/              # Event monitoring & action triggers
â”‚   â”‚   â”‚   â”œâ”€â”€ event-listener.ts  # On-chain event subscription
â”‚   â”‚   â”‚   â”œâ”€â”€ file-watcher.ts    # Local file change monitoring
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ handlers/              # Event handlers that trigger actions
â”‚   â”‚   â”‚   â”œâ”€â”€ stake-handler.ts   # Delegated stake changes
â”‚   â”‚   â”‚   â”œâ”€â”€ provider-handler.ts # Provider key management
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ actions/               # Action executors (reuse CLI commands)
â”‚   â”‚   â”‚   â”œâ”€â”€ update-coinbase.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ create-provider.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ add-keys.ts
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ state/                 # Persistent state management
â”‚   â”‚       â”œâ”€â”€ checkpoint.ts      # Last processed block tracking
â”‚   â”‚       â””â”€â”€ pending-actions.ts # Action queue with retry logic
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                      # Shared core functionality
â”‚   â”‚   â”œâ”€â”€ components/            # External service clients
â”‚   â”‚   â”‚   â”œâ”€â”€ aztec-client.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ ethereum-client.ts
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”œâ”€â”€ config/                # Configuration management
â”‚   â”‚   â”‚   â”œâ”€â”€ schema.ts          # Config validation schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.ts          # Config loading & persistence
â”‚   â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”‚   â”œâ”€â”€ utils/                 # Shared utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ file-operations.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ logger.ts          # Structured logging
â”‚   â”‚   â”‚   â””â”€â”€ errors.ts          # Custom error types
â”‚   â”‚   â””â”€â”€ types.ts               # Shared type definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ index.ts                   # Main entry point - mode selection
â”‚   â””â”€â”€ types.ts                   # Top-level types
â”‚
â”œâ”€â”€ config/                        # Config file templates & examples
â”‚   â”œâ”€â”€ example.env
â”‚   â””â”€â”€ prometheus.yml             # Example Prometheus config
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ cli-usage.md
â”‚   â””â”€â”€ server-setup.md
â”‚
â”œâ”€â”€ scripts/                       # Build & deployment scripts
â”‚   â””â”€â”€ build.sh
â”‚
â”œâ”€â”€ test/                          # Tests (mirror src structure)
â”‚   â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ server/
â”‚   â””â”€â”€ core/
â”‚
â”œâ”€â”€ .editorconfig
â”œâ”€â”€ .gitignore
â”œâ”€â”€ eslint.config.js
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ PLAN.md
```

---

## Key Design Principles

### 1. Separation of Concerns

- **CLI**: Thin wrapper around core functionality, one-shot execution
- **Server/Scrapers**: Fetch data from sources (files, RPC, contracts)
- **Server/Metrics**: Expose scraped data as Prometheus metrics
- **Server/Watchers**: Monitor for changes and trigger handlers
- **Server/Handlers**: Orchestrate actions based on events
- **Server/Actions**: Execute business logic (reuse CLI commands where possible)
- **Core**: Pure business logic, no I/O assumptions

### 2. Shared Core & Code Reuse

- Both CLI and server modes use the same `core/` functions
- Server actions should reuse CLI command implementations where applicable
- Scrapers can be used by both metrics and watchers
- Single source of truth for business logic ensures consistency

### 3. Single Server Process with Multiple Responsibilities

The server mode orchestrates multiple concurrent activities:

- HTTP server for `/metrics` endpoint (Prometheus scraping)
- HTTP server for `/health` endpoint (health checks)
- Periodic scrapers to refresh metrics
- Event listeners for on-chain changes
- File watchers for local state changes
- Action executor with retry logic

### 4. Configuration Strategy

```typescript
// Base config used by all modes
interface BaseConfig {
  aztecNodeUrl: string;
  ethereumNodeUrl: string;
  aztecDockerDir: string;
  providerAdminAddress?: string;
}

// CLI uses only base config
interface CliConfig extends BaseConfig {}

// Server mode combines all monitoring & metrics config
interface ServerConfig extends BaseConfig {
  // HTTP server
  port: number;

  // Scraping/metrics
  scrapeInterval: number;

  // Event watching
  pollInterval: number;
  startBlock?: number;

  // State persistence
  stateDir: string;

  // Action execution
  autoExecuteActions: boolean;
  maxRetries: number;
}
```

---

## Migration Path

### Phase 1: Restructure Existing Code âœ… COMPLETE

1. âœ… Move current commands to `cli/commands/`
2. âœ… Extract clients to `core/components/`
3. âœ… Move config logic to `core/config/`
4. âœ… Update imports

### Phase 2: Implement Server Infrastructure

1. Create `server/` directory structure
2. Implement base scraper interface
3. Setup HTTP server with `/metrics` and `/health` endpoints
4. Add structured logging (pino)
5. Implement graceful shutdown handling

### Phase 3: Implement Scrapers & Metrics

1. Implement node scraper (files + RPC)
2. Implement L1 contract scraper
3. Define Prometheus metrics (node-metrics.ts, l1-metrics.ts)
4. Wire scrapers to metrics registry
5. Add periodic scraper execution

### Phase 4: Implement Event Watching

1. Implement on-chain event listener (viem)
2. Add file system watcher for local changes
3. Implement state management (checkpoint tracking)
4. Create event handlers (stake, provider changes)
5. Wire handlers to action executors

### Phase 5: Action Execution & State

1. Implement action executors (reuse CLI commands)
2. Add action queue with retry logic
3. Implement state persistence (checkpoint + pending actions)
4. Add dry-run mode for testing
5. Implement action execution safeguards

### Phase 6: Testing & Documentation

1. Write unit tests for core components
2. Write integration tests (mock external services)
3. Test against local devnet if available
4. Document server configuration
5. Document deployment strategies
6. Add example docker-compose setup

---

## Technology Recommendations

### CLI Parsing

- **commander** or **yargs** for argument parsing
- Better than manual argv parsing as complexity grows

### Logging

- **pino** - fast, structured logging
- Different log levels per mode
- JSON output for production

### Prometheus Client

- **prom-client** - standard Node.js Prometheus client
- Built-in collectors for node metrics

### Event Listening

- Use **viem** public client (already a dependency)
- `watchEvent()` for real-time listening
- `getLogs()` with checkpoint for catch-up

### State Persistence (Watcher)

- Start simple: JSON file for checkpoint
- Consider SQLite if state grows complex
- Store: last processed block, pending actions

---

## Server Mode Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SERVER PROCESS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   Scrapers   â”‚          â”‚   Watchers   â”‚                 â”‚
â”‚  â”‚              â”‚          â”‚              â”‚                 â”‚
â”‚  â”‚ â€¢ Node RPC   â”‚          â”‚ â€¢ On-chain   â”‚                 â”‚
â”‚  â”‚ â€¢ Node files â”‚          â”‚   events     â”‚                 â”‚
â”‚  â”‚ â€¢ L1 state   â”‚          â”‚ â€¢ File       â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   changes    â”‚                 â”‚
â”‚         â”‚                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                         â”‚                          â”‚
â”‚         â–¼                         â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   Metrics    â”‚          â”‚   Handlers   â”‚                 â”‚
â”‚  â”‚   Registry   â”‚          â”‚              â”‚                 â”‚
â”‚  â”‚              â”‚          â”‚ Detect if    â”‚                 â”‚
â”‚  â”‚ Store latest â”‚          â”‚ action neededâ”‚                 â”‚
â”‚  â”‚ values       â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚                          â”‚
â”‚         â”‚                         â–¼                          â”‚
â”‚         â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â”‚                  â”‚   Actions    â”‚                 â”‚
â”‚         â”‚                  â”‚              â”‚                 â”‚
â”‚         â”‚                  â”‚ Execute CLI  â”‚                 â”‚
â”‚         â”‚                  â”‚ commands     â”‚                 â”‚
â”‚         â”‚                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                         â”‚                          â”‚
â”‚         â”‚                         â–¼                          â”‚
â”‚         â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â”‚                  â”‚    State     â”‚                 â”‚
â”‚         â”‚                  â”‚              â”‚                 â”‚
â”‚         â”‚                  â”‚ Checkpoint + â”‚                 â”‚
â”‚         â”‚                  â”‚ Pending queueâ”‚                 â”‚
â”‚         â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ HTTP Server  â”‚                                           â”‚
â”‚  â”‚              â”‚                                           â”‚
â”‚  â”‚ /metrics     â”‚ â—„â”€â”€â”€â”€ Prometheus scrapes                 â”‚
â”‚  â”‚ /health      â”‚ â—„â”€â”€â”€â”€ Health checks                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Open Questions to Consider

1. **Action execution policy** ğŸ¤”
   - Start with automatic execution + dry-run flag
   - Add approval mechanism later if needed
   - Implement retry with exponential backoff
   - Max 3 retries, then alert and pause

2. **Multi-node support** ğŸ¤”
   - Phase 1: One instance per node (simpler)
   - Phase 2: Could support multiple nodes in single instance
   - Config would specify multiple node endpoints

3. **Alert mechanisms** ğŸ¤”
   - Primary: Rely on Prometheus alerting (standard practice)
   - Optional: Direct alerts for critical failures
   - Log errors prominently for monitoring tools

4. **Security** ğŸ”’
   - Store private keys in environment variables
   - Support keystore files with password
   - Consider hardware wallet integration for production
   - Actions should be signed locally, not via RPC

5. **Metrics to expose** ğŸ“Š
   - Node health (is synced, current block, peer count)
   - Validator status (is active, stake amount)
   - Provider status (key count, coinbase address)
   - L1 contract state (total stake, provider count)
   - Action metrics (success/failure count, retry count)

---

## Next Steps

1. âœ… Review and finalize architecture (combined server mode)
2. Decide on CLI framework (commander/yargs)
3. âœ… Phase 1 restructuring complete
4. Start Phase 2: Server infrastructure
5. Implement incrementally, testing each component

---

## Benefits of Combined Server Approach

âœ… **Single Process**: Simpler deployment and monitoring
âœ… **Resource Efficient**: Shared connections and in-memory state
âœ… **Code Reuse**: Scrapers serve both metrics and watchers
âœ… **Unified Configuration**: One config file for all server functionality
âœ… **Graceful Degradation**: If one component fails, restart handles all
âœ… **Appropriate Scale**: Perfect for single-node management use case
âœ… **Future Flexibility**: Can split later if requirements change (YAGNI)

## Example Usage

```bash
# CLI mode - one-shot commands
aztec-butler deposit-calldata --amount 1000
aztec-butler get-publisher-eth
aztec-butler get-create-provider-calldata

# Server mode - long-running daemon
aztec-butler serve

# Server with custom config
aztec-butler serve --config /path/to/config.json

# Server with dry-run (no actions executed)
aztec-butler serve --dry-run
```
